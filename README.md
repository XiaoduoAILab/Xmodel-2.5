# Xmodel-2.5

[English Version](#english-version) | [ä¸­æ–‡ç‰ˆæœ¬](#ä¸­æ–‡ç‰ˆæœ¬)

---

## ä¸­æ–‡ç‰ˆæœ¬

# Xmodel-2.5: é¢å‘è½»é‡çº§æ™ºèƒ½ä½“çš„1.3Bå‚æ•°æ¨ç†æ¨¡å‹

<div align="center">

![Xmodel-2.5](images/model_table.png)

**ä¸€ä¸ªä¸“ä¸ºè¾¹ç¼˜è®¾å¤‡å’Œæˆæœ¬æ•æ„Ÿåœºæ™¯è®¾è®¡çš„1.3Bå‚æ•°å°å‹è¯­è¨€æ¨¡å‹**

[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE)
[![GitHub stars](https://img.shields.io/github/stars/XiaoduoAILab/Xmodel-2.5)](https://github.com/XiaoduoAILab/Xmodel-2.5/stargazers)

</div>

## ğŸ“– æ¦‚è¿°

Xmodel-2.5 æ˜¯ä¸€ä¸ª1.3Bå‚æ•°çš„è§£ç å™¨ä¸“ç”¨è¯­è¨€æ¨¡å‹ï¼Œä¸“é—¨ä¸ºè½»é‡çº§æ™ºèƒ½ä½“åº”ç”¨è®¾è®¡ã€‚ç›¸æ¯”ä¼ ç»Ÿ7-13Bå‚æ•°çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒXmodel-2.5åœ¨ä¿æŒå¼ºå¤§æ¨ç†èƒ½åŠ›çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—èµ„æºéœ€æ±‚ï¼Œä½¿å…¶æˆä¸ºè¾¹ç¼˜è®¾å¤‡å’Œæˆæœ¬æ•æ„Ÿéƒ¨ç½²åœºæ™¯çš„ç†æƒ³é€‰æ‹©ã€‚

### ğŸš€ ä¸»è¦ç‰¹æ€§

- **é«˜æ•ˆæ¶æ„**: 1.3Bå‚æ•°ï¼Œ48å±‚ï¼Œ1536éšè—ç»´åº¦ï¼Œæ”¯æŒ131Kä¸Šä¸‹æ–‡é•¿åº¦
- **å…ˆè¿›è®­ç»ƒæŠ€æœ¯**: é‡‡ç”¨æœ€å¤§æ›´æ–°å‚æ•°åŒ–(Î¼P)ã€FP8æ··åˆç²¾åº¦è®­ç»ƒå’ŒWarmup-Stable-Decayè¯¾ç¨‹å­¦ä¹ 
- **å¼ºå¤§æ¨ç†èƒ½åŠ›**: åœ¨1-2Bå‚æ•°æ¨¡å‹ä¸­ï¼Œåœ¨å¤æ‚æ¨ç†å’Œäº¤äº’å¼ä»»åŠ¡ä¸Šè¾¾åˆ°æœ€ä¼˜æ€§èƒ½
- **å¤šè¯­è¨€æ”¯æŒ**: æ”¯æŒä¸­è‹±æ–‡ï¼Œåœ¨ä¸­æ–‡ç†è§£ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚
- **å¼€æºè®¸å¯**: Apache 2.0è®¸å¯è¯ï¼Œå®Œå…¨å¼€æº

## ğŸ—ï¸ æ¨¡å‹æ¶æ„

Xmodel-2.5é‡‡ç”¨æ·±åº¦-ç˜¦èº«è§£ç å™¨ä¸“ç”¨æ¶æ„ï¼š

| å‚æ•° | å€¼ |
|------|-----|
| å‚æ•°é‡ | 1.3B |
| éšè—å±‚ç»´åº¦ | 1536 |
| å±‚æ•° | 48 |
| æ³¨æ„åŠ›å¤´æ•° | 24 |
| KVå¤´æ•°(GQA) | 8 |
| å‰é¦ˆç½‘ç»œç»´åº¦ | 3840 |
| æœ€å¤§åºåˆ—é•¿åº¦ | 3712 |
| ä½ç½®ç¼–ç  | RoPE (base=500000) |
| è¯æ±‡è¡¨å¤§å° | 129,280 (DeepSeek-v3åˆ†è¯å™¨) |

## ğŸ“Š æ€§èƒ½è¡¨ç°

### æ¨ç†ä»»åŠ¡è¡¨ç°

| ä»»åŠ¡ | Xmodel-2.5 | åŒè§„æ¨¡æœ€ä½³æ¨¡å‹ |
|------|-------------|----------------|
| ARC-Challenge | 46.16% | +0.66% |
| MMLU | 49.98% | +1.23% |
| GSM8K | 56.56% | +14.56% |
| C-Eval | 43.16% | +8.59% |
| CMMLU | 44.29% | +9.77% |

### äº¤äº’å¼æ™ºèƒ½ä½“ä»»åŠ¡

| ä»»åŠ¡ | æˆåŠŸç‡ |
|------|---------|
| HotpotQA | 13.7% |
| FEVER | 40.0% |
| AlfWorld | 7.8% |
| WebShop | 2.2% |

## ğŸ› ï¸ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

- Python 3.8+
- PyTorch 2.0+
- CUDA 11.8+
- Megatron-LM

### å®‰è£…

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/XiaoduoAILab/Xmodel-2.5.git
cd Xmodel-2.5

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### è®­ç»ƒç¤ºä¾‹

```bash
# è¿è¡ŒåŸºçº¿è®­ç»ƒè„šæœ¬
cd examples/xmodel
bash baseline.sh
```

### ä½¿ç”¨Hugging Face Transformers

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from models.configuration_xmodel2 import XmodelConfig
from models.modeling_xmodel2 import XmodelForCausalLM

# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained("XiaoduoAILab/Xmodel-2.5")
model = AutoModelForCausalLM.from_pretrained("XiaoduoAILab/Xmodel-2.5")

# æ¨ç†ç¤ºä¾‹
input_text = "ä¸­å›½çš„é¦–éƒ½æ˜¯"
inputs = tokenizer(input_text, return_tensors="pt")
outputs = model.generate(**inputs, max_length=50)
print(tokenizer.decode(outputs[0]))
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
Xmodel-2.5/
â”œâ”€â”€ models/                    # æ¨¡å‹å®šä¹‰
â”‚   â”œâ”€â”€ configuration_xmodel2.py
â”‚   â””â”€â”€ modeling_xmodel2.py
â”œâ”€â”€ examples/                  # ä½¿ç”¨ç¤ºä¾‹
â”‚   â””â”€â”€ xmodel/
â”‚       â”œâ”€â”€ baseline.sh
â”‚       â”œâ”€â”€ fp8.sh
â”‚       â””â”€â”€ data_config.json
â”œâ”€â”€ data_preprocess/           # æ•°æ®é¢„å¤„ç†è„šæœ¬
â”œâ”€â”€ hpo/                       # è¶…å‚æ•°ä¼˜åŒ–
â”œâ”€â”€ tools/                     # å·¥å…·è„šæœ¬
â”œâ”€â”€ docs/                      # æ–‡æ¡£
â””â”€â”€ pretrain_gpt.py           # ä¸»è®­ç»ƒè„šæœ¬
```

## ğŸ”§ è®­ç»ƒé…ç½®

### è®­ç»ƒé˜¶æ®µ

1. **é¢„çƒ­é˜¶æ®µ(Warmup)**: 2kæ­¥ï¼Œå­¦ä¹ ç‡çº¿æ€§ä¸Šå‡
2. **ç¨³å®šé˜¶æ®µ(Stable)**: 530kæ­¥ï¼Œæ‰¹é‡å¤§å°é€æ­¥å¢åŠ 
3. **è¡°å‡é˜¶æ®µ(Decay)**: 40kæ­¥ï¼Œæ··åˆé«˜è´¨é‡SFTæ•°æ®

### æ•°æ®ç»„æˆ

- **ç¨³å®šé˜¶æ®µ**: å¤šæ ·åŒ–é¢„è®­ç»ƒæ•°æ®ï¼ŒåŒ…å«13ä¸ªæ•°æ®æº
- **è¡°å‡é˜¶æ®µ**: é«˜è´¨é‡æ•™å­¦æ•°æ®ï¼ŒåŒ…å«10ä¸ªæ•°æ®æºï¼Œ63.88%ä¸ºSFTæ··åˆæ•°æ®

## ğŸ“„ å¼•ç”¨

å¦‚æœæ‚¨åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº†Xmodel-2.5ï¼Œè¯·å¼•ç”¨æˆ‘ä»¬çš„è®ºæ–‡ï¼š

```bibtex
@article{liu2025xmodel,
  title={Xmodel-2.5: 1B-scale Agentic Reasoner with Stable and Efficient Pre-training},
  author={Liu, Yang and Zhong, Xiaolong and Jiang, Ling},
  journal={arXiv preprint},
  year={2025}
}
```

## ğŸ¤ è´¡çŒ®

æˆ‘ä»¬æ¬¢è¿ç¤¾åŒºè´¡çŒ®ï¼è¯·æŸ¥çœ‹[è´¡çŒ®æŒ‡å—](CONTRIBUTING.md)äº†è§£å¦‚ä½•å‚ä¸ã€‚

## ğŸ“œ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨Apache 2.0è®¸å¯è¯ - è¯¦è§[LICENSE](LICENSE)æ–‡ä»¶ã€‚

## ğŸ“ è”ç³»æˆ‘ä»¬

- é‚®ç®±: {zhongxiaolong,liuyangfoam}@xiaoduotech.com
- GitHub Issues: [é—®é¢˜åé¦ˆ](https://github.com/XiaoduoAILab/Xmodel-2.5/issues)

---

## English Version

# Xmodel-2.5: 1.3B Parameter Reasoning Model for Lightweight Agents

<div align="center">

![Xmodel-2.5](images/model_table.png)

**A 1.3B parameter small language model designed for edge devices and cost-sensitive scenarios**

[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE)
[![GitHub stars](https://img.shields.io/github/stars/XiaoduoAILab/Xmodel-2.5)](https://github.com/XiaoduoAILab/Xmodel-2.5/stargazers)

</div>

## ğŸ“– Overview

Xmodel-2.5 is a 1.3B parameter decoder-only language model specifically designed for lightweight agent applications. Compared to traditional 7-13B parameter large language models, Xmodel-2.5 maintains strong reasoning capabilities while significantly reducing computational requirements, making it an ideal choice for edge devices and cost-sensitive deployment scenarios.

### ğŸš€ Key Features

- **Efficient Architecture**: 1.3B parameters, 48 layers, 1536 hidden dimensions, supports 131K context length
- **Advanced Training Techniques**: Utilizes Maximal Update Parameterization (Î¼P), FP8 mixed-precision training, and Warmup-Stable-Decay curriculum learning
- **Strong Reasoning Capabilities**: Achieves state-of-the-art performance in complex reasoning and interactive tasks among 1-2B parameter models
- **Multilingual Support**: Supports Chinese and English, with excellent performance on Chinese understanding tasks
- **Open Source License**: Apache 2.0 license, fully open source

## ğŸ—ï¸ Model Architecture

Xmodel-2.5 adopts a deep-and-thin decoder-only architecture:

| Parameter | Value |
|-----------|-------|
| Parameters | 1.3B |
| Hidden Size | 1536 |
| Layers | 48 |
| Attention Heads | 24 |
| KV Heads (GQA) | 8 |
| FFN Dimension | 3840 |
| Max Sequence Length | 3712 |
| Position Encoding | RoPE (base=500000) |
| Vocabulary Size | 129,280 (DeepSeek-v3 tokenizer) |

## ğŸ“Š Performance

### Reasoning Task Performance

| Task | Xmodel-2.5 | Best in Class (1-2B) |
|------|-------------|----------------------|
| ARC-Challenge | 46.16% | +0.66% |
| MMLU | 49.98% | +1.23% |
| GSM8K | 56.56% | +14.56% |
| C-Eval | 43.16% | +8.59% |
| CMMLU | 44.29% | +9.77% |

### Interactive Agent Tasks

| Task | Success Rate |
|------|--------------|
| HotpotQA | 13.7% |
| FEVER | 40.0% |
| AlfWorld | 7.8% |
| WebShop | 2.2% |

## ğŸ› ï¸ Quick Start

### Requirements

- Python 3.8+
- PyTorch 2.0+
- CUDA 11.8+
- Megatron-LM

### Installation

```bash
# Clone repository
git clone https://github.com/XiaoduoAILab/Xmodel-2.5.git
cd Xmodel-2.5

# Install dependencies
pip install -r requirements.txt
```

### Training Example

```bash
# Run baseline training script
cd examples/xmodel
bash baseline.sh
```

### Using Hugging Face Transformers

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from models.configuration_xmodel2 import XmodelConfig
from models.modeling_xmodel2 import XmodelForCausalLM

# Load model and tokenizer
tokenizer = AutoTokenizer.from_pretrained("XiaoduoAILab/Xmodel-2.5")
model = AutoModelForCausalLM.from_pretrained("XiaoduoAILab/Xmodel-2.5")

# Inference example
input_text = "The capital of China is"
inputs = tokenizer(input_text, return_tensors="pt")
outputs = model.generate(**inputs, max_length=50)
print(tokenizer.decode(outputs[0]))
```

## ğŸ“ Project Structure

```
Xmodel-2.5/
â”œâ”€â”€ models/                    # Model definitions
â”‚   â”œâ”€â”€ configuration_xmodel2.py
â”‚   â””â”€â”€ modeling_xmodel2.py
â”œâ”€â”€ examples/                  # Usage examples
â”‚   â””â”€â”€ xmodel/
â”‚       â”œâ”€â”€ baseline.sh
â”‚       â”œâ”€â”€ fp8.sh
â”‚       â””â”€â”€ data_config.json
â”œâ”€â”€ data_preprocess/           # Data preprocessing scripts
â”œâ”€â”€ hpo/                       # Hyperparameter optimization
â”œâ”€â”€ tools/                     # Utility scripts
â”œâ”€â”€ docs/                      # Documentation
â””â”€â”€ pretrain_gpt.py           # Main training script
```

## ğŸ”§ Training Configuration

### Training Phases

1. **Warmup Phase**: 2k steps, linear learning rate increase
2. **Stable Phase**: 530k steps, gradually increasing batch size
3. **Decay Phase**: 40k steps, mixed with high-quality SFT data

### Data Composition

- **Stable Phase**: Diverse pre-training data from 13 sources
- **Decay Phase**: High-quality instructional data from 10 sources, 63.88% SFT mixed data

## ğŸ“„ Citation

If you use Xmodel-2.5 in your research, please cite our paper:

```bibtex
@article{liu2025xmodel,
  title={Xmodel-2.5: 1B-scale Agentic Reasoner with Stable and Efficient Pre-training},
  author={Liu, Yang and Zhong, Xiaolong and Jiang, Ling},
  journal={arXiv preprint},
  year={2025}
}
```

## ğŸ¤ Contributing

We welcome community contributions! Please check the [Contributing Guidelines](CONTRIBUTING.md) for details on how to participate.

## ğŸ“œ License

This project is licensed under the Apache 2.0 License - see the [LICENSE](LICENSE) file for details.

## ğŸ“ Contact

- Email: {zhongxiaolong,liuyangfoam}@xiaoduotech.com
- GitHub Issues: [Issue Tracker](https://github.com/XiaoduoAILab/Xmodel-2.5/issues)
